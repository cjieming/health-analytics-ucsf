{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics in Healthcare\n",
    "\n",
    "## Week 1 - Data Aquisition\n",
    "\n",
    "Objectives: \n",
    "\n",
    "- Learn to get data from an API\n",
    "- Use a scrapy spider to crawl a website\n",
    "- Use Pandas to format JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Into to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the the data from from the API \n",
    "\n",
    "Go the the website https://clinicaltrialsapi.cancer.gov/v1/ and explore the API. Download data from the trial given as an example (NCT02194738) and view the data. The function given to you used the GET function from the requests library in Python. pprint will allow you to view the JSON in a formatted manner. Be careful, these responses can be large!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_response_json(api_args, api_root=\"https://clinicaltrialsapi.cancer.gov/v1/\"):\n",
    "    \"\"\"\n",
    "    This function returns the json of a GET response\n",
    "    \n",
    "    arguments:\n",
    "    api_root -- str, the root website of the API\n",
    "    api_args -- str, the arguements to the API\n",
    "    \n",
    "    returns\n",
    "    json response, str\n",
    "    \"\"\"\n",
    "    return requests.get(api_root + api_args).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Query the API using arguments\n",
    "\n",
    "Read the documentation for the API on the website, and retrieve the data to get all trial IDs for all trials in NY where the eligibility criteria was restricted to women and whose primary purpose was basic science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Structure the JSON data\n",
    "\n",
    "Pandas is a very useful python library that allows you to view data in tabular form. It is very good (and fast!) for viewing CSV file and even connecting to databases. For this exercise, we shall use it to structure the JSON data in a more readable format.\n",
    "\n",
    "Hint: use the function - pd.io.json.json_normalize (This will help it to read data from nested JSON files i.e. where dictionaries and lists are embedded within others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scrape Data using a spider\n",
    "\n",
    "In this part, we shall use Scrapy (another python library) to create a \"Spider\" to crawl the websites to gather data.\n",
    "Scrapy documentation has a nice tutorial to get started at https://doc.scrapy.org/en/latest/intro/tutorial.html. We shall be modifying the code here for our purpose.\n",
    "\n",
    "To get started you can type \"!scrapy startproject tutorial\" in a new cell. This allows you to run command line arguements from within Jupyter, which can often be handy. The command will create a directory structure that can be used by your Scrapy commands.\n",
    "\n",
    "The code from the tutorial is below:\n",
    "\n",
    "Your objective are:\n",
    "\n",
    "1. Create a meaningful query (use the query from Part 2, if nothing comes to mind) to get data from the API\n",
    "2. Then get ALL the data from the trials retrieved by your query, and save them in individual files.\n",
    "\n",
    "Be sure to change the name of the variable as well the class to something meaninful, and use \"scrapy _spiderName_\" from the command line to run the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - METAL https://github.com/Kitware/MetaIO\n",
    " - DICOM – Waveform An extension of Dicom for storing waveform data\n",
    " - European Data Format (.edf, .rec)\n",
    " - CSV (.csv),XML,TXT\n",
    " - ecgML – A markup language for electrocardiogram data acquisition and analysis.\n",
    " - EDF/EDF+ – European Data Format.\n",
    " - FEF – File Exchange Format for Vital signs, CEN TS 14271.\n",
    " - GDF v1.x – The General Data Format for biomedical signals – Version 1.x.\n",
    " - GDF v2.x – The General Data Format for biomedical signals – Version 2.x.\n",
    " - HL7aECG – Health Level 7 v3 annotated ECG.\n",
    " - MFER – Medical waveform Format Encoding Rules\n",
    " - OpenXDF – Open Exchange Data Format\n",
    " - TDMS (.tdms)\n",
    " - LVM (.lvm)\n",
    " - SCP-ECG – Standard Communication Protocol for Computer assisted electrocardiography EN1064:2007,\n",
    " - SIGIF – A digital SIGnal Interchange Format with application in neurophysiology.\n",
    " - WFDB – Format of Physiobank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response_json = get_response_json()\n",
    "# # print(response_json)\n",
    "# df = pd.io.json.json_normalize(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pprint(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response_json['anatomic_sites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for column in df.columns:\n",
    "#     print(column, df[column].dtype)\n",
    "# #     print(df[column][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # df_sites = pd.io.json.json_normalize(df.sites[0])\n",
    "# df_eligibility = pd.io.json.json_normalize(df[\"eligibility.unstructured\"][0])\n",
    "# df_diseases = pd.io.json.json_normalize(df[\"diseases\"][0])\n",
    "# df_arms = pd.io.json.json_normalize(df[\"arms\"][0])\n",
    "\n",
    "# df.drop([\"sites\", \"eligibility.unstructured\", \"diseases\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for column in df.columns:\n",
    "#     print(column, df[column].dtype)\n",
    "#     print(df[column][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_eligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_diseases.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(response_json['trials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.io.json.json_normalize(response_json['trials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for trial_id in df.nct_id:\n",
    "#     print(get_response_json(api_args=\"clinical-trial/\" + trial_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
